{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we try 2 different approahces for getting the stock name or ticker from each WSB post. \n",
    "1. Named-entity recognition (NER)\n",
    "2. REGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Entities:\n",
      "GM: 1706\n",
      "AMC: 699\n",
      "GME: 361\n",
      "U: 74\n",
      "G: 60\n",
      "UMC: 58\n",
      "AM: 55\n",
      "T: 53\n",
      "O: 50\n",
      "K: 43\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read DF\n",
    "df = pd.read_csv(\"cleaned_unlabeled.csv\").text_cleaned\n",
    "\n",
    "# Initialize the NER pipeline (using BERT)\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "# Reconstruct and clean entities\n",
    "def reconstruct_entities(ner_results):\n",
    "    entities = []\n",
    "    current_entity = {\"entity\": None, \"text\": \"\", \"start\": None, \"end\": None}\n",
    "    \n",
    "    for token in ner_results:\n",
    "        if token[\"entity\"].startswith(\"B-\"):  \n",
    "            if current_entity[\"text\"]:  \n",
    "                entities.append(current_entity)\n",
    "            # Start a new entity\n",
    "            current_entity = {\n",
    "                \"entity\": token[\"entity\"][2:],  \n",
    "                \"text\": token[\"word\"].replace(\"##\", \"\"),  \n",
    "                \"start\": token[\"start\"],\n",
    "                \"end\": token[\"end\"],\n",
    "            }\n",
    "        elif token[\"entity\"].startswith(\"I-\") and current_entity[\"entity\"]: \n",
    "            current_entity[\"text\"] += token[\"word\"].replace(\"##\", \"\")  \n",
    "            current_entity[\"end\"] = token[\"end\"] \n",
    "\n",
    "    # Append the last entity\n",
    "    if current_entity[\"text\"]:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract and clean entities from the sentences\n",
    "all_entities = []\n",
    "for sentence in df:\n",
    "    # Skip empty values\n",
    "    if pd.isna(sentence) or sentence == \"\":\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        ner_results = ner_pipeline(sentence)\n",
    "        reconstructed = reconstruct_entities(ner_results)\n",
    "        # Clean and normalize the text of each entity\n",
    "        for entity in reconstructed:\n",
    "            cleaned_text = entity[\"text\"].strip().upper() \n",
    "            all_entities.append(cleaned_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentence: {sentence}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Count occurrences of each entity\n",
    "entity_counts = Counter(all_entities)\n",
    "\n",
    "# Get the top 10 entities\n",
    "top_10_entities = entity_counts.most_common(10)\n",
    "print(\"\\nTop 10 Entities:\")\n",
    "for entity, count in top_10_entities:\n",
    "    print(f\"{entity}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51216/51216 [00:24<00:00, 2078.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total mentions per company:\n",
      "GME      13717\n",
      "AMC       4872\n",
      "HOOD      4763\n",
      "BB        2119\n",
      "TSLA      1023\n",
      "PLTR       873\n",
      "AMZN       520\n",
      "AAPL       505\n",
      "SNDL       380\n",
      "F          372\n",
      "AMD        347\n",
      "SPCE       313\n",
      "MSFT       227\n",
      "NIO        225\n",
      "TLRY       194\n",
      "SOFI       191\n",
      "NVDA       145\n",
      "COIN       135\n",
      "DKNG       119\n",
      "BBBY       111\n",
      "PLUG       101\n",
      "MRNA        97\n",
      "SNAP        92\n",
      "PFE         84\n",
      "ZM          81\n",
      "GOOGL       39\n",
      "LCID        23\n",
      "META        18\n",
      "RIVN        17\n",
      "CHPT        16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_company_mentions(df, companies_list):\n",
    "    \"\"\"\n",
    "    Scan text for company names and tickers.\n",
    "    \"\"\"\n",
    "    # Create patterns for companies\n",
    "    company_patterns = {}\n",
    "    for company, ticker in companies_list:\n",
    "        patterns = [\n",
    "            rf'\\b{re.escape(company)}\\b',  # Full company name\n",
    "            rf'\\${ticker}\\b',              # $TICKER\n",
    "            rf'\\b{ticker}\\b'               # TICKER without $\n",
    "        ]\n",
    "        company_patterns[(company, ticker)] = re.compile('|'.join(patterns), re.IGNORECASE)\n",
    "    \n",
    "    # Process each row\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = str(row['text_cleaned'])\n",
    "        \n",
    "        # Find all mentioned companies in this text\n",
    "        mentioned_companies = []\n",
    "        for (company, ticker), pattern in company_patterns.items():\n",
    "            if pattern.search(text):\n",
    "                mentioned_companies.append(ticker)\n",
    "        \n",
    "        # Create row with original text and mentioned companies\n",
    "        row_dict = {\n",
    "            'original_text': text,\n",
    "            'mentioned_companies': ','.join(mentioned_companies) if mentioned_companies else ''\n",
    "        }\n",
    "        results.append(row_dict)\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Count total mentions for each company\n",
    "    all_mentions = []\n",
    "    for row in result_df['mentioned_companies']:\n",
    "        if row:\n",
    "            all_mentions.extend(row.split(','))\n",
    "    mention_counts = pd.Series(all_mentions).value_counts()\n",
    "    \n",
    "    print(\"\\nTotal mentions per company:\")\n",
    "    print(mention_counts)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Top 30 companies:\n",
    "wallstreetbets_companies = [\n",
    "    (\"GameStop\", \"GME\"),\n",
    "    (\"AMC Entertainment\", \"AMC\"),\n",
    "    (\"Tesla\", \"TSLA\"),\n",
    "    (\"NVIDIA\", \"NVDA\"),\n",
    "    (\"AMD\", \"AMD\"),\n",
    "    (\"Apple\", \"AAPL\"),\n",
    "    (\"Microsoft\", \"MSFT\"),\n",
    "    (\"Palantir\", \"PLTR\"),\n",
    "    (\"BlackBerry\", \"BB\"),\n",
    "    (\"Bed Bath & Beyond\", \"BBBY\"),\n",
    "    (\"Virgin Galactic\", \"SPCE\"),\n",
    "    (\"Ford\", \"F\"),\n",
    "    (\"Meta\", \"META\"),\n",
    "    (\"Amazon\", \"AMZN\"),\n",
    "    (\"Alphabet\", \"GOOGL\"),\n",
    "    (\"Rivian\", \"RIVN\"),\n",
    "    (\"Lucid\", \"LCID\"),\n",
    "    (\"SoFi\", \"SOFI\"),\n",
    "    (\"Snapchat\", \"SNAP\"),\n",
    "    (\"Robinhood\", \"HOOD\"),\n",
    "    (\"Coinbase\", \"COIN\"),\n",
    "    (\"Moderna\", \"MRNA\"),\n",
    "    (\"Pfizer\", \"PFE\"),\n",
    "    (\"Zoom\", \"ZM\"),\n",
    "    (\"DraftKings\", \"DKNG\"),\n",
    "    (\"Tilray\", \"TLRY\"),\n",
    "    (\"Sundial Growers\", \"SNDL\"),\n",
    "    (\"NIO\", \"NIO\"),\n",
    "    (\"Plug Power\", \"PLUG\"),\n",
    "    (\"ChargePoint\", \"CHPT\")\n",
    "]\n",
    "\n",
    "# Read your CSV\n",
    "df = pd.read_csv(\"cleaned_unlabeled.csv\")\n",
    "\n",
    "# Find mentions\n",
    "result_df = find_company_mentions(df, wallstreetbets_companies)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "result_df.to_csv('company_mentions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>mentioned_companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its not about the money its about sending a me...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math professor scott steiner says the numbers ...</td>\n",
       "      <td>GME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exit the system the ceo of nasdaq pushed to ha...</td>\n",
       "      <td>GME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new sec filing for gme! can someone less retar...</td>\n",
       "      <td>GME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not to distract from gme just thought our amc ...</td>\n",
       "      <td>GME,AMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51211</th>\n",
       "      <td>what i learned investigating sava fud spreader...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51212</th>\n",
       "      <td>daily popular tickers thread for august 02 202...</td>\n",
       "      <td>GME,AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51213</th>\n",
       "      <td>hitler reacts to the market being irrational</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51214</th>\n",
       "      <td>daily discussion thread for august 02 2021 you...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51215</th>\n",
       "      <td>fraternal association of gambling gentlemen an...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           original_text mentioned_companies\n",
       "0      its not about the money its about sending a me...                    \n",
       "1      math professor scott steiner says the numbers ...                 GME\n",
       "2      exit the system the ceo of nasdaq pushed to ha...                 GME\n",
       "3      new sec filing for gme! can someone less retar...                 GME\n",
       "4      not to distract from gme just thought our amc ...             GME,AMC\n",
       "...                                                  ...                 ...\n",
       "51211  what i learned investigating sava fud spreader...                    \n",
       "51212  daily popular tickers thread for august 02 202...             GME,AMD\n",
       "51213       hitler reacts to the market being irrational                    \n",
       "51214  daily discussion thread for august 02 2021 you...                    \n",
       "51215  fraternal association of gambling gentlemen an...                    \n",
       "\n",
       "[51216 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See new DF\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
